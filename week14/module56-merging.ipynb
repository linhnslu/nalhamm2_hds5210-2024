{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DataFrames Together\n",
    "\n",
    "In this module, we're going to talk about two different types of merging: concatenation and masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "To \"concatenate\" means to combine things end-to-end.  That is, we're going to merge together multiple data sets in a way that we just keep appending more rows end-on-end.\n",
    "\n",
    "In `https://hds5210-data.s3.amazonaws.com/drinking/` there are a whole list of files that we want to merge together into a single data frame.  They all have the same format, but the are from different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# *** WARNING ***\n",
    "# Do not run this code on your local machine if you have the AWS CLI already configured\n",
    "# It could cause a problem with your existing security credentials\n",
    "# and permantently erase your existing access keys and secrets\n",
    "\n",
    "# If you're curious about what this code does, \n",
    "# it creates a file called ~/.aws/credentials with credentials I've created\n",
    "# that allow you to list files in a particular AWS s3 storage bucket.\n",
    "\n",
    "mkdir -p ~/.aws\n",
    "grep hds5210 ~/.aws/credentials 2>/dev/null || cat >>~/.aws/credentials <<EOF \n",
    "[hds5210]\n",
    "aws_access_key_id = AKIAUXBOKEFK63ZPGD62\n",
    "aws_secret_access_key = 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\n",
    "aws_default_region = us-east-1\n",
    "EOF\n",
    "chmod 644 ~/.aws/credentials\n",
    "cat ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then this one-liner gets a list of the files in a specific storage\n",
    "# bucket subfolder and writes that list of files to a files.txt file.\n",
    "# After you run this code, you should see a file in Google Colab\n",
    "# with this same name.  From there, we'll use Python code to get the files.\n",
    "!aws --profile hds5210 s3 ls s3://hds5210-data/drinking/ >files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function we'll use to read all of the file names from that\n",
    "# text file that the aws command above created.\n",
    "# The command above outputs in a \"human readable\" format that we have to parse\n",
    "# making some assumptions (like file names won't have spaces in them).  It \n",
    "# only works because this specific subfolder doesn't have any files with spaces\n",
    "# in the name.\n",
    "\n",
    "def get_files(listing_file):\n",
    "  files = []\n",
    "\n",
    "  # Open the listing file\n",
    "  with open(listing_file) as f:\n",
    "    for line in f.readlines():\n",
    "      # Split based on space, grab the last item, strip off extra newline\n",
    "      name = line.split(' ')[-1].strip()\n",
    "      # The aws command returns an empty-name file as well for some reason\n",
    "      # So, we'll strip that out\n",
    "      if len(name) > 0:\n",
    "        files.append(name)\n",
    "\n",
    "  # Return the list of files\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files('files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, let's read each of those files into their own df and store that in a list of dfs\n",
    "dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    df = pd.read_csv('https://hds5210-data.s3.amazonaws.com/drinking/'+f)\n",
    "    print(f'Read {f}')\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can concatenate them together with pd.concat\n",
    "drinking = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure the counts match up...\n",
    "\n",
    "Length of combined dataframe == Sum of the length of the individual dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(x) for x in dataframes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to label the rows as they get concatenated together.  That can be handy if you want to keep track of which input file each row came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinking2 = pd.concat(dataframes, keys=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinking2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinking2.head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinking2.index.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Side-by-Side\n",
    "\n",
    "The stacking example above is more common, but it might be interesting to concatenate data side-by-side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1=[['Paul','Boal'],['Anny', 'Monroe'],['Eric','Westhus'],['Andy','Slavitt']]\n",
    "names2=[['Paul Boal'],['Anny Monroe'],['Eric Westhus'],[''],['Mario Garza']]\n",
    "n1 = pd.DataFrame(names1, columns=['First','Last'])\n",
    "n2 = pd.DataFrame(names2, columns=['Full Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([n1,n2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "With \"masking\", we are taking two data sets and overlaying one ontop of the other.  If the first has values, then those will be kept.  If the first has a blank (NaN), then the underlying value from the next data set will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes1 = pd.read_csv('https://hds5210-data.s3.amazonaws.com/nppes1.csv')\n",
    "nppes2 = pd.read_csv('https://hds5210-data.s3.amazonaws.com/nppes2.csv')\n",
    "nppes1.set_index('NPI', inplace=True)\n",
    "nppes2.set_index('NPI', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes1['State'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nppes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nppes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes1[pd.isnull(nppes1['State'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = nppes1.combine_first(nppes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['State'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nppes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc[1225590060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppes1.loc[1225590060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
